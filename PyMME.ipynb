{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc62e1c",
   "metadata": {},
   "source": [
    "# PyMME Tutorial\n",
    "\n",
    "This is an example PyMME script, showing how to use XCast to construct, assess, and compare Multi-Model Ensemble forecasts using different statistical techniques. It also demonstrates Xarray data manipulation. Please refer to the [XCast](https://github.com/kjhall01/xcast/blob/main/XCAST_DOCS.md) and [Xarray](http://xarray.pydata.org/en/stable/) documentation for even more detail on the functions used. \n",
    "\n",
    "\n",
    "### Data Ingestion & Preparation\n",
    "In this first section, we import Xarray and XCast, as well as a plotting utility called PyPlot. We then use Xarray to load our example data from NetCDF files, and to get it into an appropriate format for XCast. For this example, we will make MME forecasts using prepared NCEP-CFSv2, CanSIPSv2, and COLA-RSMAS-CCSM4 precipitation forecasts in NetCDF format as predictors, and CPC-CMAP-URD precipitation as a predictand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e88c9d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import src as xc    # This line will become \"import xcast as xc\" once xcast is installed with anaconda\n",
    "import xarray as xr  #import xarray\n",
    "import matplotlib.pyplot as plt  #need this for multi-plotting, for now.\n",
    "import cartopy.crs as ccrs # geomapping library\n",
    "\n",
    "# Opening example datasets:\n",
    "#    - Each dataset is only three-dimensional - sample x lat x lon - so they need to be \n",
    "#      concatenated along a new axis, \"feature,\" which will have the name, \"M\". \n",
    "#    - We use 'decode_times=False' because the NetCDF files' time-coordinates don't adhere \n",
    "#      to the proper standards. Other files won't necessarily require that. \n",
    "obs = xr.open_dataset('test_data/cpc_jaso_asia_full.nc', decode_times=False)\n",
    "cola = xr.open_dataset('test_data/cola_jaso_asia_full.nc', decode_times=False)\n",
    "cansipsv2 = xr.open_dataset('test_data/cansipsv2_jaso_asia_full.nc', decode_times=False)\n",
    "cfsv2 = xr.open_dataset('test_data/cfsv2_jaso_asia_full.nc', decode_times=False)\n",
    "\n",
    "\n",
    "# Preparing Predictand Dataset:\n",
    "#   - Since \"Obs\" is only 3D, (sample x lat x lon) it needs a fourth, 'feature' dimension\n",
    "#     obs.expand_dims({'M':[0]}) creates a new dimension of size one with the name 'M', \n",
    "#     and assigns it the coordinate 0. \n",
    "#\n",
    "#   - These datasets cover a large geographical range, from Latitude 40:180, and longitude -10:90. \n",
    "#     We will use Y.sel(X=slice(65,95), Y=slice(0,40)) to limit the geographical range to lon 65:95,\n",
    "#     and lat 0:40. \n",
    "Y = obs.expand_dims({'M':[0]})\n",
    "Y = Y.sel(X=slice(65,95), Y=slice(0, 40)) * 3\n",
    "\n",
    "# Preparing Predictor Dataset: \n",
    "#   - Again, since cola, cfsv2, and cansipsv2 all lack 'feature' dimensions (see XCast Dimensionality in the docs)\n",
    "#     we need to create that by concatenating the three datasets along a new dimension called 'M', and assigning it\n",
    "#     coordinates. This is accomplished with xr.concat([cola, cfsv2, cansipsv2], 'M').assign_coords({'M':[0,1,2]})\n",
    "#\n",
    "#   - Next, since our predictor data is available from 1982 to 2021, and our predictand data is available from 1982\n",
    "#     to 2014, we need to truncate our predictor datasets to 2014. XCast requires there be the same number of samples \n",
    "#     in the predictor and predictand datasets. This is accomplished with the X.isel(S=slice(None, -7)), which cuts off \n",
    "#     the last 7 indices of data along the 'S' dimension, which represents samples/years here. \n",
    "X = xr.concat([cola, cfsv2, cansipsv2], 'M').assign_coords({'M':[0,1,2]})\n",
    "X_TRAIN = X.isel(S=slice(None, -7)).sel(X=slice(65,95), Y=slice(0,40))\n",
    "X_TEST = X.isel(S=slice(-7, None))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb9afa9",
   "metadata": {},
   "source": [
    "### Cross Validation & Skill\n",
    "In this section, we will use the data we prepared above to cross-validate each method of interest to us. In this tutorial, we will examine hindcasts made with Bias-Corrected Ensemble Mean, Extreme Learning Machine, Random Forest, Multiple Linear Regression, and Multi-Layer Perceptron. We have made some choices for expediency, like limiting MLP's backpropagation algorith to 10 iterations, and using a cross-validation window of 5, so its possible that we could find better skill with more time. \n",
    "\n",
    "ELM, Random Forest, and Multi-Layer Perceptron are non-deterministic methods, meaning that they are randomly initialized, and will give different results each time. For consistency's sake, we will run them each multiple times (ND=10) and take the average. This can also give us a measure of uncertainty (ND-Variation). You can see that the dataset returned by the 'cross_validate' method gives us BOTH hindcasts (hindcast mean), and stddev (hindcast variation).  \n",
    "\n",
    "The 'Gridpoint-Wise' approach can also be hard to interpret spatially, so we will apply 9x9 gaussian smoothing to make things easier to interpret. We will then use the 'smoothed' results to calculate some skill metrics using XCast's 'xc.deterministic_skill' function, which uses XSkillScore's skill metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CROSSVALIDATING BiasCorrectedEnsembleMean:  2021-09-17 09:40:50.097592 [*************************]\n",
      " CROSSVALIDATING ExtremeLearningMachine:  2021-09-17 09:41:31.986907 [*************************]\n",
      " CROSSVALIDATING RandomForest:  2021-09-17 09:41:32.050599 [                         ]\r"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client \n",
    "#client = Client(processes=False)\n",
    "\n",
    "# Cross Validate each method\n",
    "em = xc.cross_validate(xc.BiasCorrectedEnsembleMean, X_TRAIN.prec, Y.prate, x_sample_dim='S', verbose=2, window=5, ND=10)\n",
    "elm = xc.cross_validate(xc.ExtremeLearningMachine, X_TRAIN.prec, Y.prate, x_sample_dim='S', verbose=2, window=5, ND=10, lat_chunks=4, lon_chunks=4, hidden_layer_size=5)\n",
    "rf = xc.cross_validate(xc.RandomForest, X_TRAIN.prec, Y.prate, x_sample_dim='S', verbose=2, window=5, ND=10, lat_chunks=4, lon_chunks=4 )\n",
    "mlp = xc.cross_validate(xc.MultiLayerPerceptron, X_TRAIN.prec, Y.prate, x_sample_dim='S', verbose=2, window=5, ND=1, max_iter=10, lat_chunks=4, lon_chunks=4)\n",
    "mlr = xc.cross_validate(xc.MultipleLinearRegression, X_TRAIN.prec, Y.prate, x_sample_dim='S', verbose=2, window=5, ND=1, lat_chunks=4, lon_chunks=4)\n",
    "\n",
    "#em = xr.open_dataset('test_data/ensemble_mean_xval_hcst.nc', decode_times=False)\n",
    "#elm = xr.open_dataset('test_data/extreme_learning_machine_xval_hcst.nc', decode_times=False)\n",
    "#rf = xr.open_dataset('test_data/random_forest_xval_hcst.nc', decode_times=False)\n",
    "#mlr = xr.open_dataset('test_data/multiple_linear_regression_xval_hcst.nc', decode_times=False)\n",
    "#mlp = xr.open_dataset('test_data/multilayer_perceptron_xval_hcst.nc', decode_times=False)\n",
    "\n",
    "# Apply smoothing to each result's hcst-mean & std-dev\n",
    "em_sm = em.hindcasts #xc.gaussian_smooth(em.hindcasts, x_sample_dim='S')\n",
    "elm_sm = elm.hindcasts #xc.gaussian_smooth(elm.hindcasts, x_sample_dim='S')\n",
    "rf_sm = rf.hindcasts #xc.gaussian_smooth(rf.hindcasts, x_sample_dim='S')\n",
    "mlr_sm = mlr.hindcasts # xc.gaussian_smooth(mlr.hindcasts, x_sample_dim='S')\n",
    "mlp_sm = mlp.hindcasts # xc.gaussian_smooth(mlp.hindcasts, x_sample_dim='S')\n",
    "\n",
    "em_sm_sd = em.nd_stddev #xc.gaussian_smooth(em.nd_stddev, x_sample_dim='S')\n",
    "elm_sm_sd = elm.nd_stddev # xc.gaussian_smooth(elm.nd_stddev, x_sample_dim='S')\n",
    "rf_sm_sd =  rf.nd_stddev #xc.gaussian_smooth(rf.nd_stddev, x_sample_dim='S')\n",
    "mlr_sm_sd = mlr.nd_stddev #xc.gaussian_smooth(mlr.nd_stddev, x_sample_dim='S')\n",
    "mlp_sm_sd = mlp.nd_stddev #xc.gaussian_smooth(mlp.nd_stddev, x_sample_dim='S')\n",
    "\n",
    "# Calculate skill of each smoothed hindcast mean against the predictand dataset\n",
    "em_skill = xc.deterministic_skill(em_sm, Y.prate, x_sample_dim='S')\n",
    "elm_skill = xc.deterministic_skill(elm_sm, Y.prate, x_sample_dim='S')\n",
    "rf_skill = xc.deterministic_skill(rf_sm, Y.prate, x_sample_dim='S')\n",
    "mlr_skill = xc.deterministic_skill(mlr_sm, Y.prate, x_sample_dim='S')\n",
    "mlp_skill = xc.deterministic_skill(mlp_sm, Y.prate, x_sample_dim='S')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683da5e8",
   "metadata": {},
   "source": [
    "### Predictands and Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [[Y.prate.mean('T').mean('M').sel(X=slice(65,95), Y=slice(0,40)), cansipsv2.prec.mean('S').sel(X=slice(65,95), Y=slice(0,40)), cfsv2.prec.mean('S').sel(X=slice(65,95), Y=slice(0,40)), cola.prec.mean('S').sel(X=slice(65,95), Y=slice(0,40))],\n",
    "        [Y.prate.std('T').mean('M').sel(X=slice(65,95), Y=slice(0,40)),  cansipsv2.prec.std('S').sel(X=slice(65,95), Y=slice(0,40)), cfsv2.prec.std('S').sel(X=slice(65,95), Y=slice(0,40)), cola.prec.std('S').sel(X=slice(65,95), Y=slice(0,40))]]\n",
    "titles = ['CPC-CMAP-URD PRATE', 'CanSIPSv2 Precipitation', 'NCEP-CFSv2 Precipitation', 'COLA-RSMAS-CCSM4 Precipitation']\n",
    "variables = ['Climatological Mean', 'Climatological Std. Dev.']\n",
    "vmax = [2000, 800]\n",
    "cmaps = ['BrBG', 'Oranges']\n",
    "fig, axes = plt.subplots(nrows=2, ncols=4, sharex=True, subplot_kw={'projection':ccrs.PlateCarree()}, figsize=(24, 12))\n",
    "\n",
    "for i in range(len(plots)):\n",
    "    axes[i][0].text(-0.35,0.5,variables[i],rotation=90,verticalalignment='center', transform=axes[i][0].transAxes)\n",
    "    for j in range(len(plots[0])):\n",
    "        axes[0][j].set_title(titles[j])\n",
    "        plots[i][j].plot(ax=axes[i][j], cmap=cmaps[i])\n",
    "        axes[i][j].coastlines()\n",
    "        gl = axes[i][j].gridlines()\n",
    "        gl.xlabels_bottom, gl.ylabels_left = True, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d1b1f1",
   "metadata": {},
   "source": [
    "### Hindcast Climatology & Variation\n",
    "Let's start by examining the mean and standard variation of the crossvalidated hindcasts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [[Y.prate.mean('T').mean('M'), em_sm.mean('S').mean('M'), elm_sm.mean('S').mean('M'), rf_sm.mean('S').mean('M'), mlr_sm.mean('S').mean('M'), mlp_sm.mean('S').mean('M')],\n",
    "        [Y.prate.std('T').mean('M'), em_sm.std('S').mean('M'), elm_sm.std('S').mean('M'), rf_sm.std('S').mean('M'), mlr_sm.std('S').mean('M'), mlp_sm.std('S').mean('M')],\n",
    "        [Y.prate.std('T').mean('M')-Y.prate.std('T').mean('M'), em_sm_sd.mean('S').mean('M'), elm_sm_sd.mean('S').mean('M'), rf_sm_sd.mean('S').mean('M'), mlr_sm_sd.mean('S').mean('M'), mlp_sm_sd.mean('S').mean('M')]]\n",
    "titles = ['Observations', 'Ensemble Mean', 'Extreme Learning Machine', 'Random Forest', 'Multiple Linear Regression', 'Multi-Layer Perceptron']\n",
    "variables = ['Climatology', 'Std. Dev.', 'ND StdDev']\n",
    "vmax = [2000, 800, 150]\n",
    "cmaps = ['BrBG', 'Oranges', 'RdBu']\n",
    "fig, axes = plt.subplots(nrows=3, ncols=6, sharex=True, subplot_kw={'projection':ccrs.PlateCarree()}, figsize=(24, 12))\n",
    "\n",
    "for i in range(len(plots)):\n",
    "    axes[i][0].text(-0.35,0.5,variables[i],rotation=90,verticalalignment='center', transform=axes[i][0].transAxes)\n",
    "    for j in range(len(plots[0])):\n",
    "        axes[0][j].set_title(titles[j])\n",
    "        plots[i][j].plot(ax=axes[i][j], cmap=cmaps[i], vmin=0, vmax=vmax[i])\n",
    "        axes[i][j].coastlines()\n",
    "        gl = axes[i][j].gridlines()\n",
    "        gl.xlabels_bottom, gl.ylabels_left = True, True\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbfeeac",
   "metadata": {},
   "source": [
    "### Skill Metrics \n",
    "Here we plot skill metrics calculated with the XCast.deterministic_skill function, between the cross-validated hindcasts and the predictand dataset. This function uses the [XSkillScore](https://xskillscore.readthedocs.io/en/stable/) deterministic functions to compute all skill scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58d870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = [[em_skill.pearson_coefficient.mean('member'), elm_skill.pearson_coefficient.mean('member'), rf_skill.pearson_coefficient.mean('member'), mlr_skill.pearson_coefficient.mean('member'), mlp_skill.pearson_coefficient.mean('member')], \n",
    "        [em_skill.root_mean_squared_error.mean('member'), elm_skill.root_mean_squared_error.mean('member'), rf_skill.root_mean_squared_error.mean('member'), mlr_skill.root_mean_squared_error.mean('member'), mlp_skill.root_mean_squared_error.mean('member')], \n",
    "        [em_skill.mean_absolute_error.mean('member'), elm_skill.mean_absolute_error.mean('member'), rf_skill.mean_absolute_error.mean('member'), mlr_skill.mean_absolute_error.mean('member'), mlp_skill.mean_absolute_error.mean('member')],\n",
    "        [em_skill.determination_coefficient.mean('member'), elm_skill.determination_coefficient.mean('member'), rf_skill.determination_coefficient.mean('member'), mlr_skill.determination_coefficient.mean('member'), mlp_skill.determination_coefficient.mean('member')]]\n",
    "titles = ['Ensemble Mean', 'Extreme Learning Machine', 'Random Forest', 'Multiple Linear Regression', 'Multi-Layer Perceptron']\n",
    "variables = ['Pearson Coefficient', 'Root Mean Squared Error', 'Mean Absolute Error', 'Determination Coefficient']\n",
    "vmins, vmaxs = [-1, 0, 0, -1], [1, 800, 400, 1]\n",
    "cmaps = ['RdBu', 'RdBu', 'RdBu', 'RdBu']\n",
    "fig, axes = plt.subplots(nrows=4, ncols=5, sharex=True, subplot_kw={'projection':ccrs.PlateCarree()}, figsize=(24, 16))\n",
    "\n",
    "for i in range(len(plots)):\n",
    "    axes[i][0].text(-0.35,0.5,variables[i],rotation=90,verticalalignment='center', transform=axes[i][0].transAxes)\n",
    "    for j in range(len(plots[0])):\n",
    "        axes[0][j].set_title(titles[j])\n",
    "        plots[i][j].plot(ax=axes[i][j], cmap=cmaps[i], vmin=vmins[i], vmax=vmaxs[i])\n",
    "        axes[i][j].coastlines()\n",
    "        gl = axes[i][j].gridlines()\n",
    "        gl.xlabels_bottom, gl.ylabels_left = True, True\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca08a2b2",
   "metadata": {},
   "source": [
    "### Point Skill & Point Hindcasts\n",
    "We can also examine skill and hindcasts at a single point or over an aggregation of a region of space. Based on the above, it looks like there should be some skill in the South of India for this forecast, so we will aggregate over the region LAT 10:15 and LON 75:80, then calculate skill. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "south_india_obs = Y.prate.sel(X=slice(75, 80), Y=slice(10,15)).mean('X').mean('Y').expand_dims({'X':[0], 'Y':[0]})\n",
    "south_india_obs.coords['T'] = em_sm.coords['S'].values\n",
    "south_india_em = em_sm.sel(X=slice(75, 80), Y=slice(10,15)).mean('X').mean('Y').expand_dims({'X':[0], 'Y':[0]})\n",
    "south_india_elm = elm_sm.sel(X=slice(75, 80), Y=slice(10,15)).mean('X').mean('Y').expand_dims({'X':[0], 'Y':[0]})\n",
    "south_india_rf = rf_sm.sel(X=slice(75, 80), Y=slice(10,15)).mean('X').mean('Y').expand_dims({'X':[0], 'Y':[0]})\n",
    "south_india_mlr = mlr_sm.sel(X=slice(75, 80), Y=slice(10,15)).mean('X').mean('Y').expand_dims({'X':[0], 'Y':[0]})\n",
    "south_india_mlp = mlp_sm.sel(X=slice(75, 80), Y=slice(10,15)).mean('X').mean('Y').expand_dims({'X':[0], 'Y':[0]})\n",
    "\n",
    "hcsts  = xr.concat([south_india_obs.rename({'T':'S'}), south_india_em, south_india_elm, south_india_rf, south_india_mlr, south_india_mlp], 'method').assign_coords({'method':['Obs', 'EM', 'ELM', 'RF', 'MLR', 'MLP']})\n",
    "x = hcsts.isel(X=0, Y=0, M=0).plot.line(x='S', hue='method')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a6132",
   "metadata": {},
   "outputs": [],
   "source": [
    "south_india_mlp.std('S').mean('X').mean('Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b9a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "em_skill_si = xc.deterministic_skill(south_india_em, south_india_obs, x_sample_dim='S').mean('lon').mean('lat').mean('member')\n",
    "elm_skill_si = xc.deterministic_skill(south_india_elm, south_india_obs, x_sample_dim='S').mean('lon').mean('lat').mean('member')\n",
    "rf_skill_si = xc.deterministic_skill(south_india_rf, south_india_obs, x_sample_dim='S').mean('lon').mean('lat').mean('member')\n",
    "mlr_skill_si = xc.deterministic_skill(south_india_mlr, south_india_obs, x_sample_dim='S').mean('lon').mean('lat').mean('member')\n",
    "mlp_skill_si = xc.deterministic_skill(south_india_mlp, south_india_obs, x_sample_dim='S').mean('lon').mean('lat').mean('member')\n",
    "\n",
    "south_india_skills = xr.concat([em_skill_si, elm_skill_si, rf_skill_si, mlr_skill_si, mlp_skill_si], 'method').assign_coords({'method':[ 'EM', 'ELM', 'RF', 'MLR', 'MLP']})\n",
    "south_india_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c557f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
